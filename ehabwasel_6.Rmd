---
title: "ehabwasel_6"
author: "Ehab"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(dplyr)
library(magrittr)
library(ggplot2)
library(foreign)
library(kableExtra)
library(janitor , warning(FALSE))
library(readr)
```

Add the following variables to the dataset:
the total number of trials for each observation (i.e., the sum of the no and yes trials for each row)
the proportion of yes trials in each row (i.e. yes divided by the total)
the log-odds of inhibition for each row (i.e. the log-odds of yes vs no)
```{r}
data <- data.frame(conc = c(0.1, 0.5, 1, 10, 20, 30, 50, 70, 80, 100, 150),
                   no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),
                   yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)
                   ) 
data
```
inspect the new columns. Do you notice anything unusual?

```{r}
data <- data %>%  
  mutate(
    total = no + yes,
    prop = yes / total,
    logit = qlogis(prop)
  )
data
```
Add a new column to your dataset containing the corrected odds.

```{r}
robustLogit <- function(x, y) log((x + 0.5) / (y + 0.5))

data <- data %>% 
  mutate(logit2 = robustLogit(yes, no))

data
```
Fit a logistic regression model where:
prop is the outcome
conc is the only predictor
the number of total trials per row are used as weights (we need this because a different number of trials can go into defining each observation of prop)
Interpret the slope estimate.

```{r}
summary(glm(prop ~ conc, 
            family = binomial, 
            weights = total, 
            data = data))
```
 A unit increase in conc increases the log-odds of inhibition by 0.0121 units, and this increase is statistically significant.

If we exponentiate the slope estimate, we can get an interpretation in odds units, but the effect becomes multiplicative instead of additive. So for every unit increase in conc, the odds of inhibition are 1.01215 times higher. Note then that odds above 1 indicate inhibition is x-times higher, while odds below 1 indicate inhibition is x-times less.


# Titanic data
```{r}
titanic <- read_csv("titanic.csv") %>% 
  mutate(Survived = as.factor(Survived),
         Sex = as.factor(Sex),
         Pclass = as.factor(Pclass))


```
nvestigate how many passengers survived in each class. You can do this visually by creating a bar plot, or by using the table() function. Search ??table for more information.

```{r}
titanic %>% 
  ggplot(aes(Pclass, fill = Survived)) +
  geom_bar(position = "dodge") +
  labs(x = "Passenger Class",
       y = "Count") +
  theme_bw()

```

The bar plot clearly shows that people in lower class were less likely to survive.
# We can also use the `prop.table()` function to investigate this. The argument `margin = 1` turns the counts to marginal proportions.
```{r}



titanic %$% 
  table(Pclass, Survived) %>% 
  prop.table(margin = 1) %>% 
  round(2)
```


```{r}
titanic %$% 
  table(Sex, Survived) %>% 
  prop.table(margin = 1) %>% 
  round(2)
```


```{r}
titanic %>% 
  ggplot(aes(Sex, fill = Survived)) +
  geom_bar(position = "dodge") +
  labs(x = "Sex",
       y = "Count") +
  theme_bw()
```


```{r}
titanic %>% 
  ggplot(aes(Age, fill = Survived)) +
  geom_histogram(colour = "white") +
  labs(x = "Age",
       y = "Count") +
  facet_wrap(~Survived) +
  theme_bw()
```
# No predictors

```{r}
glm(Survived ~ 1, 
    family = binomial, 
    data = titanic) %>% 
  summary()

```
A logistic regression without any predictors is simply modelling the log-odds of survival for the entire population (the intercept, beta0).
# The log-odds are -0.473, and the odds are $exp(-0.473) = 0.623$.

We can also get the odds from a frequency table: the probability of survival is $342/549 = 0.623$. The log-odds equals exp(beta0) = -0.473.

```{r}
titanic %>% 
  count(Survived) %>% 
  mutate(prop = prop.table(n)) %>% 
  kbl(digits = 2) %>% 
  kable_paper(bootstrap_options = "striped", full_width = FALSE)
```
# Binary predictor

```{r}

glm(Survived ~ Sex, 
    family = binomial, 
    data = titanic) %>% 
  summary()
```

# Categorical predictor (more than 2 categories)
```{r}
glm(Survived ~ Pclass, 
    family = binomial, 
    data = titanic) %>% 
  summary()

```
glm(Survived ~ Pclass, 
    family = binomial, 
    data = titanic) %>% 
  summary()



Which category is the reference group? What are their odds of survival?
 The reference group are 1st class passengers, represented by the intercept.
 The log-odds of survival for 1st class passengers is 0.5306.
 The odds are 1.70, meaning 1st class passengers are 70% more likely to survive.
 
What are the chances of survival for 2nd and 3rd class passengers?
 For 2nd class passengers, the log-odds of survival is -0.6394.
 The odds are  0.527, meaning 2nd class passengers are 47% less likely to survive than 1st class passengers.

 For 3rd class passengers, the log-odds of survival is -1.646.
 The odds are 0.188, meaning 3nd class passengers are 81% less likely to survive than 1st class passengers.
 
 
 
#Continuous predictor
```{r}

fit1 <- glm(Survived ~ Age, 
            family = binomial, 
            data = titanic)
summary(fit1)


```
# Multinomial model with an interaction term
Specify a logistic regression model Survived is the outcome and Pclass plus an interaction between Sex and Age as the predictor.
Save this model as we will return to it later.
```{r}
fit2 <- glm(Survived ~ Pclass + Sex*Age, family = binomial, data = titanic)
summary(fit2)
```
# How is the significant interaction term interpreted in this model?

```{r}
summary(fit1)

```


```{r}
summary(fit2)
```
For model 1, the null deviance is 1186.7 and the residual deviance is 1182.3 For model 2, the null deviance is 1186.66 and the residual deviance is 793.82



We can use the anova() function to perform an analysis of deviance that compares the difference in deviances between competing models.

Compare the fit of model 1 with the fit of model 2 using anova() andtest = “Chisq”`.
```{r}
anova(fit1, fit2, test = "Chisq")
```
Use the AIC() function to get the AIC value for model 1 and model 2.

```{r}
AIC(fit1, fit2)
```
Use the BIC() function to get the BIC value for model 1 and model 2.

```{r}
BIC(fit1, fit2)
```
Which model should we proceed with?
 Model 2, as it has lower residual deviance, AIC and BIC.



#Predicted probabilites
```{r}
preds <- data.frame(predict(fit2, type = "response", se.fit = TRUE))
```


```{r}
titanic$probs <- preds$fit
titanic$se    <- preds$se.fit

```


```{r}
titanic %<>% 
  mutate(ci_Lower = probs - 1.96 * se, 
         ci_Upper = probs + 1.96 * se)



titanic %>% 
  ggplot(aes(x = Age, y = probs)) + 
    geom_ribbon(aes(ymin = ci_Lower, ymax = ci_Upper, fill = Pclass), alpha = 0.2) +
    geom_line(aes(color = Pclass), lwd = 1) + 
    ylab("Probability of Survival") +
    theme_bw() +
    facet_wrap(vars(Sex))
```



